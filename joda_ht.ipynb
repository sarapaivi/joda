{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harjoitustyö\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2abaf86877d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.5.0-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (3.4.7)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
      "\u001b[K     |████████████████████████████████| 251 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 358 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.7/site-packages (from scrapy) (4.6.3)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (19.1.0)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\"\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-21.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 58.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.14.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protego>=0.1.15->scrapy) (1.14.0)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=16.0.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 297 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting priority<2.0,>=1.1.0; extra == \"http2\"\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.19)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.8)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=dd8e62d057870dd372f725885bcf21e8123ed0436a25981757f3300662224622\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=c5127027598d40209fcbec498bfcfda2356584f83c4b13b5768ec66773ce5a27\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: zope.interface, protego, hyperframe, hpack, h2, w3lib, itemadapter, cssselect, parsel, itemloaders, queuelib, pyasn1-modules, service-identity, PyDispatcher, constantly, incremental, hyperlink, Automat, priority, Twisted, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.2.0 constantly-15.1.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.2.0 itemloaders-1.0.4 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.5.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'etuovi_scraper' using template 'basic' \n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider etuovi_scraper etuovi.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "# raapija, joka kerää etuovi.com:sta ilmoituksien tiedoista\n",
    "# asuntotyypin, osoitteen, hinnan, valmistuvuoden ja pinta-alan.\n",
    "# (Kyseessä Tampereen omakotitalot)\n",
    "class EtuoviScraperSpider(scrapy.Spider):\n",
    "    name = 'etuovi_scraper'\n",
    "    allowed_domains = ['etuovi.com']\n",
    "    # assing a product-review-page url below\n",
    "    start_urls = ['https://www.etuovi.com/myytavat-asunnot/tampere?haku=M1626860813&sivu=0']\n",
    "\n",
    "  # Funkito parse(self, response) käy läpi aloitusosoitteesta eteenpäin\n",
    "  # sivuja, joilta dataa kerätään. Parse kutsuu parse_page-funktiota, jossa\n",
    "  # on määritelty varsinainen kerättävä tieto.\n",
    "  # max 5 sivua tässä esimerkissä\n",
    "    def parse(self, response):\n",
    "        for i in range(1, 6):\n",
    "            url=response.request.url[:-1]+str(i)\n",
    "            yield scrapy.Request(url, callback=self.parse_page)\n",
    "        \n",
    "    \n",
    "    def parse_page(self, response):\n",
    "        description_texts = response.css('[class=\"flexboxgrid__col-xs-12__1I1LS\"] *::text').extract()\n",
    "        size_text = response.css('[title=\"Asuinpinta-ala / kokonaispinta-ala\"]::text').extract()\n",
    "        year_text = response.css('[class=\"flexboxgrid__col-xs-3__3Kf8r flexboxgrid__col-md-4__2DYW-\"] > span::text').extract()\n",
    "        price_text = response.css('[class=\"flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-4__2DYW-\"] > span::text').extract()\n",
    "\n",
    "        for i in range(len(price_text)):\n",
    "            \n",
    "            yield {\n",
    "                'house_type' : description_texts[3*i+1],\n",
    "                'address': description_texts[3*i+3],\n",
    "                'price': price_text[i],\n",
    "                'year': year_text[i],\n",
    "                'size': size_text[i]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_text[i]=price_text[i].replace(\"\\xa0\", \"\")\n",
    "            price_text[i]=price_text[i].replace(\"\\u20ac\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-21 08:24:35 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-04-21 08:24:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.7.10 (default, Feb 26 2021, 18:47:35) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-4.14.225-169.362.amzn2.x86_64-x86_64-with-debian-10.6\n",
      "2021-04-21 08:24:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-04-21 08:24:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2021-04-21 08:24:35 [scrapy.extensions.telnet] INFO: Telnet Password: e589175ec2bd7a56\n",
      "2021-04-21 08:24:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-04-21 08:24:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-04-21 08:24:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-04-21 08:24:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-04-21 08:24:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-04-21 08:24:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-04-21 08:24:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-04-21 08:24:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://etuovi.com/> from <GET http://etuovi.com/>\n",
      "2021-04-21 08:24:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.etuovi.com/> from <GET https://etuovi.com/>\n",
      "2021-04-21 08:24:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.etuovi.com/> from <GET http://www.etuovi.com/>\n",
      "2021-04-21 08:24:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.etuovi.com/> (referer: None)\n",
      "2021-04-21 08:24:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-04-21 08:24:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 864,\n",
      " 'downloader/request_count': 4,\n",
      " 'downloader/request_method_count/GET': 4,\n",
      " 'downloader/response_bytes': 34809,\n",
      " 'downloader/response_count': 4,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 3,\n",
      " 'elapsed_time_seconds': 1.159818,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 4, 21, 8, 24, 36, 287582),\n",
      " 'httpcompression/response_bytes': 185498,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 59355136,\n",
      " 'memusage/startup': 59355136,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 4,\n",
      " 'scheduler/dequeued/memory': 4,\n",
      " 'scheduler/enqueued': 4,\n",
      " 'scheduler/enqueued/memory': 4,\n",
      " 'start_time': datetime.datetime(2021, 4, 21, 8, 24, 35, 127764)}\n",
      "2021-04-21 08:24:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider etuovi_scraper.py -o out.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
